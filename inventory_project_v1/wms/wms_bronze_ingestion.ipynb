{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be70e37f-fee8-4c4a-b7d5-0de70bba2c38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## WMS Bronze Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06db2df8-4d32-46b7-b528-c1a54ac52ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22f4007-f0b5-47bd-a5ff-6645413374b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def updateFileLookup(table_name, status):\n",
    "    fileLookup = spark.read.table(\"inventory_project.metadata.filelookup\")\n",
    "    # Filter only the specific table row\n",
    "    df = fileLookup.filter(col(\"table_name\") == table_name) \\\n",
    "                   .withColumn(\"lastRunStatus\", lit(status)) \\\n",
    "                   .withColumn(\"lastUpdatetime\", current_timestamp())\n",
    "    \n",
    "    target_table = DeltaTable.forName(spark, \"inventory_project.metadata.filelookup\")\n",
    "\n",
    "    # Perform merge (upsert)\n",
    "    target_table.alias(\"t\") \\\n",
    "        .merge(\n",
    "            df.alias(\"s\"),\n",
    "            \"t.table_name = s.table_name\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={              \n",
    "            \"lastRunStatus\": \"s.lastRunStatus\",\n",
    "            \"lastUpdatetime\": \"s.lastUpdatetime\"\n",
    "        }) \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1044d26a-7b30-4cdb-af8c-8125403824a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_bronze_data(row):\n",
    "    source_path = row['source_path']\n",
    "    source_format = row['source_format']\n",
    "    table_name = row['table_name']\n",
    "    target_table = row['target_table']\n",
    "    schema_path = f\"/Volumes/inventory_project/bronze/wms_raw/schemas/{table_name}/schema\"\n",
    "    checkpoint_path = f\"/Volumes/inventory_project/bronze/wms_raw/checkpoints/{table_name}/checkpoint\"\n",
    "\n",
    "    # Autoloader with schema enforcement + schema evolution\n",
    "    df = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", source_format)\n",
    "        .option(\"pathGlobFilter\", f\"*.{source_format}\") \\\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_path)          # schema tracking\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") # allow new columns\n",
    "        .load(source_path)\n",
    "        .withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "        .withColumn(\"ingestion_time\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    # Write to Bronze Delta table\n",
    "    (df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)  # batch-like trigger for existing files\n",
    "    .toTable(target_table))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbe5bbf-0b54-4096-b2a9-6eda0fd792f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fileLookup_df = (\n",
    "    spark.read.table(\"inventory_project.metadata.filelookup\")\n",
    "         .filter((col(\"schema\") == \"bronze\") & (col(\"description\").startswith(\"WMS\")))\n",
    ")\n",
    "fileLookup_df = fileLookup_df.collect()\n",
    "\n",
    "audit_rows = []\n",
    "for row in fileLookup_df:\n",
    "    name = row['table_name']\n",
    "    status = 'success'\n",
    "    start_time = datetime.now()\n",
    "    try:\n",
    "        print(f\"Started processing {row['table_name']}\")\n",
    "        ingest_bronze_data(row)\n",
    "    except Exception as e:\n",
    "        status = 'failed'\n",
    "        error = str(e)\n",
    "        print(f\"Error processing {row['table_name']}: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        print(f\"Finished processing {row['table_name']}\")\n",
    "        end_time = datetime.now()\n",
    "        print(f\"Notebook {name} took {end_time-start_time} seconds to run\")\n",
    "        updateFileLookup(name,status)\n",
    "        audit_rows.append(Row(table_name = name, \n",
    "                              start_time = start_time, \n",
    "                              end_time = end_time, \n",
    "                              duration = str((end_time-start_time).total_seconds()), \n",
    "                              status = status,\n",
    "                              error_message = error,\n",
    "                              created_by = user,\n",
    "                              created_date = datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f998d4-1608-405b-89a6-bc6c549cfe3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add entry to audit table\n",
    "if audit_rows:   # only if list is not empty\n",
    "    audit_df = spark.createDataFrame(audit_rows).withColumn('created_by', lit('system'))\n",
    "    audit_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\")\\\n",
    "        .saveAsTable(\"inventory_project.metadata.audit_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcfc1cb-a7a3-4fca-96cf-848f6f521378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from inventory_project.metadata.filelookup"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8028495801761545,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "wms_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
